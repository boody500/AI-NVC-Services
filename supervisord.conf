[supervisord]
nodaemon=true
user=root


[program:web]
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
command=gunicorn app:app --bind 0.0.0.0:8000 --workers 3 --threads 3 --timeout 600
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
redirect_stderr=true


[program:worker1]
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
; Run Celery in "solo" or "threads" pool to avoid forking heavy ML models
; Limit concurrency to prevent multiple big jobs eating memory
command=celery -A tasks.celery worker --loglevel=info --concurrency=4 -P threads --prefetch-multiplier=2 -n worker1
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr

[program:worker2]
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
; Run Celery in "solo" or "threads" pool to avoid forking heavy ML models
; Limit concurrency to prevent multiple big jobs eating memory
command=celery -A tasks.celery worker --loglevel=info --concurrency=4 -P threads --prefetch-multiplier=2 -n worker2
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr


[program:worker3]
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
; Run Celery in "solo" or "threads" pool to avoid forking heavy ML models
; Limit concurrency to prevent multiple big jobs eating memory
command=celery -A tasks.celery worker --loglevel=info --concurrency=4 -P threads --prefetch-multiplier=2 -n worker3
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr

